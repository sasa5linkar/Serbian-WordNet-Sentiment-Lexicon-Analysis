{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9889604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\ML_extra\\lib\\site-packages\\treetaggerwrapper.py:739: FutureWarning: Possible nested set at position 8\n",
      "  punct2find_re = re.compile(\"([^ ])([[\" + ALONEMARKS + \"])\",\n",
      "C:\\ProgramData\\Anaconda3\\envs\\ML_extra\\lib\\site-packages\\treetaggerwrapper.py:2043: FutureWarning: Possible nested set at position 152\n",
      "  DnsHostMatch_re = re.compile(\"(\" + DnsHost_expression + \")\",\n",
      "C:\\ProgramData\\Anaconda3\\envs\\ML_extra\\lib\\site-packages\\treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
      "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\ML_extra\\lib\\site-packages\\treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
      "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'swordnet' from 'srpskiwordnet' (C:\\Users\\Korisnik\\Documents\\Python\\NLTK\\srpskiwordnet.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26012\\1846212274.py\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrpskiwordnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mswordnet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_file_into_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-16\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'swordnet' from 'srpskiwordnet' (C:\\Users\\Korisnik\\Documents\\Python\\NLTK\\srpskiwordnet.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from SerbSynpretproceing import showGrid\n",
    "from SerbainTagger import SrbTreeTagger\n",
    "from srpskiwordnet import SrbWordNetReader\n",
    "from srppolsets import PolaritySets, SrbSynset2GlossTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from StemmerByNikola import stem_str\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from srpskiwordnet import swordnet\n",
    "def load_file_into_list(filename):\n",
    "    with open(filename, mode=\"r\", encoding=\"utf-16\") as file:\n",
    "        lines = [line.strip() for line in file]\n",
    "    return lines\n",
    "from sklearn.naive_bayes import ComplementNB, BernoulliNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb419c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_range = np.logspace(1, 10, 3)\n",
    "gamma_range = np.logspace(-9, 3, 6)\n",
    "alpha_range = np.logspace(-9, 3, 12)\n",
    "\n",
    "# stop_words = [\".\", \"i\", \"ili\", \"(\", \")\", \";\", \",\", \"u\", \"iz\", \"se\", \"koji\",\n",
    "#               \"na\", \"kao\", \"sa\", \"kojim\", \"koj\"]\n",
    "RES_DIR = \".\\\\resources\\\\\"\n",
    "stop_words1 = load_file_into_list(RES_DIR + \"stopwordsSRB.txt\")\n",
    "stop_words2 = load_file_into_list(RES_DIR + \"stopwordsSRB tfidf.txt\")\n",
    "stop_words3 = load_file_into_list(RES_DIR + \"stopwordsSRB tf.txt\")\n",
    "tt = SrbTreeTagger()\n",
    "stop_words4 = list()\n",
    "for word in stop_words3:\n",
    "    pom = tt.lemmarizer(word)\n",
    "    if not pom in stop_words4:\n",
    "        stop_words4.append(pom)\n",
    "swordnet.parse_all_defintions(tt.lemmarizer)\n",
    "Synset_Sentiment_set = PolaritySets(swordnet, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27346d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "Synset_Sentiment_set.addWSWN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcecd9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_seed_words = [\"dobar\",\n",
    "                       \"dobrota\",\n",
    "                       \"lep\",\n",
    "                       \"čudesno\",\n",
    "                       \"dragocen\",\n",
    "                       \"anđeoski\",\n",
    "                       \"izobilje\",\n",
    "                       \"izbavljenje\",\n",
    "                       \"tešiti\",\n",
    "                       \"ispravnost\",\n",
    "                       \"oduševiti se\",\n",
    "                       \"slast\",\n",
    "                       \"uveseljavajući\",\n",
    "                       \"napredovati\",\n",
    "                       \"proslavljen\",\n",
    "                       \"usrećiti\",\n",
    "                       \"uspešnost\"]\n",
    "\n",
    "negative_seed_words = [\"zao\",\n",
    "                       \"antipatija\",\n",
    "                       \"beda\",\n",
    "                       \"bedan\",\n",
    "                       \"bol\",\n",
    "                       \"laž\",\n",
    "                       \"lažno\",\n",
    "                       \"korupcija\",\n",
    "                       \"krivica\",\n",
    "                       \"prezreti\",\n",
    "                       \"tuga\",\n",
    "                       \"nauditi\",\n",
    "                       \"sebičnost\",\n",
    "                       \"paćeništvo\",\n",
    "                       \"ukloniti s ovog sveta\",\n",
    "                       \"masakr\",\n",
    "                       \"ratovanje\"]\n",
    "positive_seed_IDS = [\"ENG30-01828736-v\",\n",
    "                     \"ENG30-13987905-n\",\n",
    "                     \"ENG30-01777210-v\",\n",
    "                     \"ENG30-13987423-n\"]\n",
    "\n",
    "negative_seed_IDS = [\"ENG30-01510444-a\",\n",
    "                     \"ENG30-01327301-v\",\n",
    "                     \"ENG30-00735936-n\",\n",
    "                     \"ENG30-00220956-a\",\n",
    "                     \"ENG30-02463582-a\",\n",
    "                     \"ENG30-01230387-a\",\n",
    "                     \"ENG30-00193480-a\",\n",
    "                     \"ENG30-00364881-a\",\n",
    "                     \"ENG30-14213199-n\",\n",
    "                     \"ENG30-01792567-v\",\n",
    "                     \"ENG30-07427060-n\",\n",
    "                     \"ENG30-14408086-n\",\n",
    "                     \"ENG30-14365741-n\",\n",
    "                     \"ENG30-02466111-a\"]\n",
    "\n",
    "Synset_Sentiment_set.addPOSall(positive_seed_words)\n",
    "Synset_Sentiment_set.addNEGall(negative_seed_words)\n",
    "Synset_Sentiment_set.addPOSIDall(positive_seed_IDS)\n",
    "Synset_Sentiment_set.addNEGIDall(negative_seed_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Synset_Sentiment_set.updateDataFrame()\n",
    "\n",
    "Synset_Sentiment_sets = list()\n",
    "Synset_Sentiment_sets.append(Synset_Sentiment_set)\n",
    "\n",
    "for _ in range(6):\n",
    "    Synset_Sentiment_sets.append(\n",
    "        Synset_Sentiment_sets[-1].next_itteration())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd77db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"gloss\", SrbSynset2GlossTransformer()),\n",
    "        (\"vect\", HashingVectorizer(stop_words=stop_words2)),\n",
    "        (\"tfidf\", TfidfTransformer())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daba0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f7432",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = Synset_Sentiment_set.getXY()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e003eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pipeline.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e95e48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50744542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303d26a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_features_to_select = 1  # Minimum number of features to consider\n",
    "clf = LogisticRegression()\n",
    "cv = StratifiedKFold(10)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=clf,\n",
    "    step=1,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    min_features_to_select=min_features_to_select,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27d690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
